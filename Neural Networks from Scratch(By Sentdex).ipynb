{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks \n",
    "are computational models that mimic the complex functions of the human brain. The neural networks consist of interconnected nodes or neurons that process and learn from data, enabling tasks such as pattern recognition and decision making in machine learning. The article explores more about neural networks, their working, architecture and more."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d885857d0bb94c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "Neural networks are a fundamental component of machine learning, inspired by the structure and functioning of the human brain. They excel at tasks involving pattern recognition, classification, regression, and decision-making. The key elements of neural networks are nodes or neurons, which are organized into layers and interconnected to form a network.\n",
    "\n",
    "## Basic Working of Neural Networks\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The process begins with an input layer that receives the initial data. Each node in this layer represents a feature of the input.\n",
    "\n",
    "2. **Hidden Layers:**\n",
    "   - Between the input and output layers, there can be one or more hidden layers. Each node in these layers processes the input data using weights and biases, introducing non-linear transformations.\n",
    "\n",
    "3. **Weights and Biases:**\n",
    "   - Weights determine the strength of connections between neurons, while biases control the neuron's sensitivity to the input. Training the neural network involves adjusting these parameters to optimize performance.\n",
    "\n",
    "4. **Activation Function:**\n",
    "   - Neurons apply an activation function to the weighted sum of their inputs. This introduces non-linearity, enabling the network to learn complex relationships in the data.\n",
    "\n",
    "5. **Output Layer:**\n",
    "   - The final layer produces the network's output, representing the model's prediction or decision.\n",
    "\n",
    "6. **Loss Function:**\n",
    "   - A loss function measures the difference between the predicted output and the actual target. The goal during training is to minimize this loss by adjusting weights and biases.\n",
    "\n",
    "7. **Backpropagation:**\n",
    "   - The optimization process, known as backpropagation, involves iteratively adjusting weights and biases based on the calculated loss. This is typically done using optimization algorithms like stochastic gradient descent.\n",
    "\n",
    "8. **Training and Learning:**\n",
    "   - Through multiple iterations, the neural network learns to improve its predictions by adjusting its parameters. The network generalizes from the training data to make accurate predictions on new, unseen data.\n",
    "\n",
    "## Neural Network Architectures\n",
    "\n",
    "1. **Feedforward Neural Networks (FNN):**\n",
    "   - The simplest form, where information flows in one direction, from input to output.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNN):**\n",
    "   - Specialized for image processing, using convolutional layers to detect patterns and features.\n",
    "\n",
    "3. **Recurrent Neural Networks (RNN):**\n",
    "   - Suitable for sequential data, RNNs have connections that create loops, allowing information persistence over time.\n",
    "\n",
    "4. **Long Short-Term Memory Networks (LSTM) and Gated Recurrent Units (GRU):**\n",
    "   - Variants of RNNs with improved memory and training capabilities for sequential data.\n",
    "\n",
    "5. **Generative Adversarial Networks (GAN):**\n",
    "   - Comprising a generator and a discriminator, GANs generate new data instances by learning from existing data.\n",
    "\n",
    "6. **Autoencoders:**\n",
    "   - Learn efficient representations of data by compressing it into a latent space and then reconstructing the original data.\n",
    "\n",
    "## Applications of Neural Networks\n",
    "\n",
    "1. **Image and Speech Recognition:**\n",
    "   - CNNs excel in tasks like image classification and object detection, while RNNs are suitable for speech recognition.\n",
    "\n",
    "2. **Natural Language Processing (NLP):**\n",
    "   - Neural networks power language models for tasks like sentiment analysis, machine translation, and chatbots.\n",
    "\n",
    "3. **Healthcare:**\n",
    "   - Used for disease diagnosis, drug discovery, and personalized medicine based on patient data.\n",
    "\n",
    "4. **Finance:**\n",
    "   - Applied for fraud detection, risk assessment, and algorithmic trading.\n",
    "\n",
    "5. **Autonomous Vehicles:**\n",
    "   - Neural networks enable perception and decision-making in self-driving cars.\n",
    "\n",
    "## Challenges and Future Directions\n",
    "\n",
    "1. **Interpretability:**\n",
    "   - Understanding the decision-making process of complex neural networks remains a challenge.\n",
    "\n",
    "2. **Data Privacy and Bias:**\n",
    "   - Neural networks can perpetuate biases present in training data, and privacy concerns arise when dealing with sensitive information.\n",
    "\n",
    "3. **Continual Learning:**\n",
    "   - Improving the ability of neural networks to adapt to new information over time.\n",
    "\n",
    "4. **Quantum Computing Integration:**\n",
    "   - Exploring the potential of quantum neural networks for enhanced computational power.\n",
    "\n",
    "In conclusion, neural networks are versatile tools with the potential to revolutionize various industries. As research advances, addressing challenges and ethical considerations will be crucial to maximizing their benefits."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be99f579bf3c788d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your description provides a concise overview of neural networks. Let's break down the key elements you've mentioned:\n",
    "\n",
    "1. **Definition:** Neural networks are computational models inspired by the human brain, capable of learning from data and making predictions or decisions.\n",
    "\n",
    "2. **Components:**\n",
    "   - **Neurons:** The basic processing units that receive inputs and produce outputs.\n",
    "   - **Connections:** Interconnections between neurons that transmit information.\n",
    "   - **Weights:** Parameters that regulate the strength of connections, influencing the impact of one neuron on another.\n",
    "   - **Biases:** Additional parameters that account for the neuron's sensitivity to input.\n",
    "   - **Propagation Functions:** Functions that govern how information flows through the network.\n",
    "\n",
    "3. **Functionality:**\n",
    "   - **Input Processing:** Neurons receive inputs, and the network processes this information through weighted connections and biases.\n",
    "   - **Thresholds and Activation Functions:** Neurons use activation functions to determine whether they should be activated based on the weighted sum of inputs. This introduces non-linearity into the network.\n",
    "   - **Learning Rule:** The network adjusts weights and biases during the learning process to improve its performance.\n",
    "\n",
    "4. **Learning Stages:**\n",
    "   - **Input Computation:** Neurons compute the weighted sum of inputs, including biases.\n",
    "   - **Output Generation:** The network produces an output based on the computed information.\n",
    "   - **Iterative Refinement:** The network refines its performance over time through iterative learning, adjusting weights and biases.\n",
    "\n",
    "5. **Adaptability:**\n",
    "   - Neural networks can adapt and learn from data without being explicitly programmed for a specific task.\n",
    "\n",
    "6. **Applications:**\n",
    "   - Neural networks are proficient in diverse tasks, including but not limited to pattern recognition, classification, regression, and decision-making.\n",
    "\n",
    "Your description captures the essence of how neural networks operate and learn, providing a solid foundation for understanding their functionality and applications in the field of machine learning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b277c5e371c14a"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Numpy:  1.26.3\n",
      "Matplotlib:  3.8.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "print(\"Python: \" , sys.version)\n",
    "print(\"Numpy: \" , np.__version__)\n",
    "print(\"Matplotlib: \" , matplotlib.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:09:31.409988200Z",
     "start_time": "2024-02-02T06:09:31.402539400Z"
    }
   },
   "id": "ae59866c772d9060"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What are Neural Networks?\n",
    "\n",
    "Neural networks extract identifying features from data, lacking pre-programmed understanding. Network components include neurons, connections, weights, biases, propagation functions, and a learning rule. Neurons receive inputs, governed by thresholds and activation functions. Connections involve weights and biases regulating information transfer. Learning, adjusting weights and biases, occurs in three stages: input computation, output generation, and iterative refinement enhancing the network’s proficiency in diverse tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6b0388f1fb5e2d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "inputs = [1.2 , 5.1 , 2.1]\n",
    "weights = [3.1 , 2.1 , 8.7]\n",
    "bias = 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:41:25.454626200Z",
     "start_time": "2024-02-01T12:41:25.437059400Z"
    }
   },
   "id": "cb319ca529d8ef19"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7\n"
     ]
    }
   ],
   "source": [
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:41:25.754988100Z",
     "start_time": "2024-02-01T12:41:25.742093600Z"
    }
   },
   "id": "f2e10af74e7c595e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "weights = [0.2 , 0.8 , -0.5 , 1]\n",
    "bias = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:41:26.058313Z",
     "start_time": "2024-02-01T12:41:26.045321500Z"
    }
   },
   "id": "dc1b0c0ebf10899b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[3] * weights[3] + bias\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:41:26.389028800Z",
     "start_time": "2024-02-01T12:41:26.379891100Z"
    }
   },
   "id": "e729a0317d6cd191"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "\n",
    "weights1 = [0.2 , 0.8 , -0.5 , 1]\n",
    "weights2 = [0.5 , -0.91 , 0.26 , -0.5]\n",
    "weights3 = [-0.26 , -0.27 , 0.17 , 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:45:06.589902100Z",
     "start_time": "2024-02-01T12:45:06.568550100Z"
    }
   },
   "id": "7d8aa78b5f7e3c0f"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "output = [inputs[0] * weights1[0] + inputs[1] * weights1[1] + inputs[2] * weights1[2] + inputs[3] * weights1[3] + bias1 , \n",
    "          inputs[0] * weights2[0] + inputs[1] * weights2[1] + inputs[2] * weights2[2] + inputs[3] * weights2[3] + bias2 , \n",
    "          inputs[0] * weights3[0] + inputs[1] * weights3[1] + inputs[2] * weights3[2] + inputs[3] * weights3[3] + bias3 ]\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T12:48:40.965233600Z",
     "start_time": "2024-02-01T12:48:40.956252900Z"
    }
   },
   "id": "17d27ab6e6a6b432"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "weights = [ [0.2 , 0.8 , -0.5 , 1] , [0.5 , -0.91 , 0.26 , -0.5] , [-0.26 , -0.27 , 0.17 , 0.87] ]\n",
    "biases = [ 2, 3 , 0.5]\n",
    "\n",
    "layer_outputs = [] #cnt layer\n",
    "for neuron_weights , neuron_bias in zip(weights , biases):\n",
    "    neuron_output = 0 #given neuron\n",
    "    for n_input , weight in zip(inputs , neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:16:45.563224700Z",
     "start_time": "2024-02-01T14:16:45.197148800Z"
    }
   },
   "id": "133601c96eaf2cb0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_weights : [0.2, 0.8, -0.5, 1]  neuron_bias:  2\n",
      "n_input:  1  weight:  0.2\n",
      "neuron_output += n_input * weight =  0.2\n",
      "n_input:  2  weight:  0.8\n",
      "neuron_output += n_input * weight =  1.8\n",
      "n_input:  3  weight:  -0.5\n",
      "neuron_output += n_input * weight =  0.30000000000000004\n",
      "n_input:  2.5  weight:  1\n",
      "neuron_output += n_input * weight =  2.8\n",
      "\n",
      " Neuron_output after adding bias:  4.8 \n",
      "\n",
      "neuron_weights : [0.5, -0.91, 0.26, -0.5]  neuron_bias:  3\n",
      "n_input:  1  weight:  0.5\n",
      "neuron_output += n_input * weight =  0.5\n",
      "n_input:  2  weight:  -0.91\n",
      "neuron_output += n_input * weight =  -1.32\n",
      "n_input:  3  weight:  0.26\n",
      "neuron_output += n_input * weight =  -0.54\n",
      "n_input:  2.5  weight:  -0.5\n",
      "neuron_output += n_input * weight =  -1.79\n",
      "\n",
      " Neuron_output after adding bias:  1.21 \n",
      "\n",
      "neuron_weights : [-0.26, -0.27, 0.17, 0.87]  neuron_bias:  0.5\n",
      "n_input:  1  weight:  -0.26\n",
      "neuron_output += n_input * weight =  -0.26\n",
      "n_input:  2  weight:  -0.27\n",
      "neuron_output += n_input * weight =  -0.8\n",
      "n_input:  3  weight:  0.17\n",
      "neuron_output += n_input * weight =  -0.29000000000000004\n",
      "n_input:  2.5  weight:  0.87\n",
      "neuron_output += n_input * weight =  1.8849999999999998\n",
      "\n",
      " Neuron_output after adding bias:  2.385 \n",
      "\n",
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "weights = [ [0.2 , 0.8 , -0.5 , 1] , [0.5 , -0.91 , 0.26 , -0.5] , [-0.26 , -0.27 , 0.17 , 0.87] ]\n",
    "biases = [ 2, 3 , 0.5]\n",
    "\n",
    "layer_outputs = [] #cnt layer\n",
    "\n",
    "for neuron_weights , neuron_bias in zip(weights , biases):\n",
    "    print(\"neuron_weights :\" , neuron_weights , \" neuron_bias: \" , neuron_bias)\n",
    "    neuron_output = 0 #given neuron\n",
    "    for n_input , weight in zip(inputs , neuron_weights):\n",
    "        print(\"n_input: \" , n_input , \" weight: \" , weight)\n",
    "        neuron_output += n_input * weight\n",
    "        print(f\"neuron_output += n_input * weight = \" , neuron_output)\n",
    "    neuron_output += neuron_bias\n",
    "    print(\"\\n Neuron_output after adding bias: \" , neuron_output , \"\\n\")\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:30:09.244145600Z",
     "start_time": "2024-02-01T14:30:09.222791500Z"
    }
   },
   "id": "63ae72f354476e40"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.35 \n",
      " 1.2\n"
     ]
    }
   ],
   "source": [
    "some_value = 0.5\n",
    "weight = -0.7\n",
    "bias = 0.7\n",
    "print(some_value * weight ,\"\\n\" , some_value + bias)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:30:55.788296100Z",
     "start_time": "2024-02-01T14:30:55.764098300Z"
    }
   },
   "id": "dc860444efa2d738"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dot Product\n",
    "using the NumPy library to calculate dot products for neural network inputs. The dot product is a fundamental operation in neural networks, often used to compute the weighted sum of inputs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48d92568f655e0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "weights = [0.2 , 0.8 , -0.5 , 1]\n",
    "bias = 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:10:19.688219800Z",
     "start_time": "2024-02-01T15:10:19.674218700Z"
    }
   },
   "id": "ff92767dd91c9cd"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(weights , inputs) + bias\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:10:19.991286100Z",
     "start_time": "2024-02-01T15:10:19.976147800Z"
    }
   },
   "id": "28b6e958ec0d72ad"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "inputs = [1 , 2 , 3 , 2.5]\n",
    "weights = [ [0.2 , 0.8 , -0.5 , 1] ,\n",
    "            [0.5 , -0.91 , 0.26 , -0.5] ,\n",
    "            [-0.26 , -0.27 , 0.17 , 0.87] ]\n",
    "biases = [ 2, 3 , 0.5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:12:19.624370900Z",
     "start_time": "2024-02-01T15:12:19.607741100Z"
    }
   },
   "id": "e37e4affc4455eaf"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(weights , inputs) + biases\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:12:19.844047800Z",
     "start_time": "2024-02-01T15:12:19.836083400Z"
    }
   },
   "id": "2bb1be05a19d3bd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7c3a4c31df64854"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "inputs = [[1 , 2 , 3 , 2.5] ,\n",
    "          [2 , 5 , -1 , 2] , \n",
    "          [-1.5 , 2.7 , 3.3 , -0.8]]\n",
    "\n",
    "weights = [ [0.2 , 0.8 , -0.5 , 1] ,\n",
    "            [0.5 , -0.91 , 0.26 , -0.5] ,\n",
    "            [-0.26 , -0.27 , 0.17 , 0.87] ]\n",
    "\n",
    "biases = [ 2, 3 , 0.5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:31:11.034800700Z",
     "start_time": "2024-02-01T15:31:11.011269400Z"
    }
   },
   "id": "278e5e1b502def3e"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m biases\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "output = np.dot(weights , inputs) + biases\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:31:13.145886900Z",
     "start_time": "2024-02-01T15:31:13.082779900Z"
    }
   },
   "id": "10f0bb44cbdb3323"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(inputs , np.array(weights).T ) + biases\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:54:01.226580800Z",
     "start_time": "2024-02-01T15:54:01.191145200Z"
    }
   },
   "id": "5d12d2586e3bea99"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    9.9   -0.09 ]\n",
      " [ 0.21  -1.81  -1.449]\n",
      " [ 3.885  2.7    0.026]]\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(weights , np.array(inputs).T ) + biases\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:00:14.542118Z",
     "start_time": "2024-02-01T16:00:14.391618400Z"
    }
   },
   "id": "87d2221f17f4812e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# adding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f13448c5cc384b"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "\n",
    "weights2 = [ [0.1 , -0.14, 0.5] ,\n",
    "            [-0.5 , 0.12 , -0.33] ,\n",
    "            [-0.44 , 0.73 , -0.13] ]\n",
    "\n",
    "biases2 = [ -1 , 2 , -0.5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T05:45:34.313911900Z",
     "start_time": "2024-02-02T05:45:34.296341300Z"
    }
   },
   "id": "3fd7e629c88f4b36"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "layer1_output = np.dot(inputs , np.array(weights).T ) + biases\n",
    "layer2_output = np.dot(layer1_output , np.array(weights2).T ) + biases2\n",
    "\n",
    "print(layer2_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T05:45:35.247289700Z",
     "start_time": "2024-02-02T05:45:35.228173200Z"
    }
   },
   "id": "e5d5d78de8ea572a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**In this code:**\n",
    "\n",
    "- layer1_output calculates the output of the first layer using the weights (weights) and biases (biases). This is similar to what you did in the previous code.\n",
    "\n",
    "- layer2_output then takes the output of the first layer (layer1_output) and calculates the output of the second layer using the new weights (weights2) and biases (biases2).\n",
    "\n",
    "This sequence of operations represents a basic two-layer neural network. The output of the first layer serves as the input to the second layer, and the final result is stored in layer2_output. This is a common structure in neural networks where layers are sequentially stacked to model complex relationships in data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd5608fd5c92583"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Object layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78ff053ac15f0273"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "X = [[ 1 , 2 , 3 , 2.5] , \n",
    "     [ 2 , 5 , -1 , 2] ,\n",
    "     [-1.5 , 2.7 , 3.3 , -0.8]] #Training dataset\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T05:53:40.787180200Z",
     "start_time": "2024-02-02T05:53:40.737386200Z"
    }
   },
   "id": "2ba91520f5ac9861"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self , n_input , n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_input , n_neurons)\n",
    "        self.biases = np.zeros((1 , n_neurons))\n",
    "    def forward(self , inputs):\n",
    "        self.output = np.dot(inputs , self.weights) + self.biases\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:18:42.207642Z",
     "start_time": "2024-02-02T06:18:42.184745400Z"
    }
   },
   "id": "13ff4c35047b7c4d"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "layer1 = Layer_Dense( 4 , 5 )\n",
    "layer2 = Layer_Dense( 5 , 2 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:20:11.470690100Z",
     "start_time": "2024-02-02T06:20:11.444183900Z"
    }
   },
   "id": "fb907a7ed75a59fc"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10007759 -0.5409994   0.20834961 -0.80242381  0.16287552]\n",
      " [-0.54525363 -1.07406753 -0.16094813 -0.73773363  0.03245683]\n",
      " [ 0.28777291 -0.27399245  0.66923335 -0.36859649  0.11569085]]\n"
     ]
    }
   ],
   "source": [
    "layer1.forward(X)\n",
    "print(layer1.output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:20:31.384202100Z",
     "start_time": "2024-02-02T06:20:31.352865Z"
    }
   },
   "id": "40aa45a8dff22644"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09455211  0.14520825]\n",
      " [-0.10087607  0.10206715]\n",
      " [-0.00252688  0.2137255 ]]\n"
     ]
    }
   ],
   "source": [
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:21:07.888015Z",
     "start_time": "2024-02-02T06:21:07.855126400Z"
    }
   },
   "id": "77090faa925189dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Here's the breakdown:**\n",
    "\n",
    "- Layer_Dense is a class that represents a dense layer in a neural network.\n",
    "\n",
    "- The __init__ method initializes the layer with random weights and zero biases.\n",
    "- The forward method calculates the output of the layer using the dot product of inputs and weights, adding biases.\n",
    "- layer1 is an instance of the Layer_Dense class with 4 input neurons and 5 output neurons.\n",
    "\n",
    "- layer2 is another instance with 5 input neurons (matching the output neurons of layer1) and 2 output neurons.\n",
    "\n",
    "- layer1.forward(X) calculates the output of the first layer using the training dataset X.\n",
    "\n",
    "- layer2.forward(layer1.output) calculates the final output of the neural network by passing the output of the first layer through the second layer.\n",
    "\n",
    "This code demonstrates a basic feedforward pass through a neural network with two layers. It's a great starting point for understanding the implementation of neural networks in code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3a5010fda998a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Certainly! Here are some interview questions related to the topics you've covered:\n",
    "\n",
    "### Neural Networks Basics:\n",
    "\n",
    "1. **Question:** What is a neural network, and how does it relate to the human brain?\n",
    "   - **Answer:** A neural network is a computational model inspired by the human brain, consisting of interconnected nodes that process and learn from data. It's used for tasks like pattern recognition and decision-making in machine learning.\n",
    "\n",
    "2. **Question:** Explain the components of a neural network.\n",
    "   - **Answer:** Components include neurons, connections, weights, biases, activation functions, and a learning rule. Neurons process inputs using weights and biases, and the learning rule adjusts these parameters during training.\n",
    "\n",
    "3. **Question:** What is the purpose of the activation function in a neural network?\n",
    "   - **Answer:** The activation function introduces non-linearity to the model, enabling it to learn complex relationships in the data and make more sophisticated predictions.\n",
    "\n",
    "### NumPy and Dot Product:\n",
    "\n",
    "4. **Question:** How does NumPy facilitate operations in neural networks, and why is it commonly used?\n",
    "   - **Answer:** NumPy is a powerful numerical library in Python that provides efficient array operations. It is used in neural networks for matrix and vector computations, making it easy to perform operations like the dot product.\n",
    "\n",
    "5. **Question:** Explain the purpose of the dot product in neural networks.\n",
    "   - **Answer:** The dot product is used to calculate the weighted sum of inputs, a crucial step in computing the output of a neuron. It simplifies matrix multiplication in neural network layers.\n",
    "\n",
    "### Layer Implementation:\n",
    "\n",
    "6. **Question:** Describe the purpose of the `Layer_Dense` class in your code.\n",
    "   - **Answer:** The `Layer_Dense` class represents a dense layer in a neural network. It initializes random weights and zero biases, and the `forward` method computes the layer's output using the dot product of inputs and weights plus biases.\n",
    "\n",
    "7. **Question:** Why is the activation function not included in the `Layer_Dense` class?\n",
    "   - **Answer:** The activation function is typically applied separately after the dot product in the forward pass. This allows for flexibility, as different layers or even neurons within the same layer may use different activation functions.\n",
    "\n",
    "### Batch Processing:\n",
    "\n",
    "8. **Question:** Why is batch processing important in neural networks?\n",
    "   - **Answer:** Batch processing allows the network to process multiple inputs simultaneously, improving computational efficiency and facilitating parallelization. It also helps in achieving better convergence during training.\n",
    "\n",
    "9. **Question:** Compare the first and second examples of batch processing. What issue did you address in the second example?\n",
    "   - **Answer:** In the second example, transposing the weights matrix was necessary to ensure the compatibility of dimensions for matrix multiplication, resolving a dimensionality issue in the first example.\n",
    "\n",
    "These questions cover a range of topics related to neural networks, NumPy, dot product, layer implementation, and batch processing. Be prepared to discuss the concepts, rationale behind code implementations, and the significance of each step in the neural network development process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2837d0281a1ba01e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weights and Biases:\n",
    "\n",
    "1. **Theoretical Question:** Explain the role of weights in a neural network and why they are crucial for learning.\n",
    "   - **Answer:** Weights determine the strength of connections between neurons, influencing the impact of one neuron on another. During training, the network adjusts weights to learn patterns and relationships in the data.\n",
    "\n",
    "2. **Practical Question:** What happens if all weights in a neural network are initialized to the same value?\n",
    "   - **Answer:** If all weights are initialized to the same value, the neurons will essentially perform the same computations during training, making it difficult for the network to learn distinct features. It's essential to initialize weights with some randomness to break symmetry.\n",
    "\n",
    "### Inputs and Outputs:\n",
    "\n",
    "1. **Conceptual Question:** Explain the concept of input and output layers in a neural network.\n",
    "   - **Answer:** The input layer receives the initial data, and the output layer produces the final predictions or outputs. Neurons in the input layer represent features, and neurons in the output layer represent the predicted classes or values.\n",
    "\n",
    "2. **Practical Question:** In a classification task, why is the softmax activation function commonly used in the output layer?\n",
    "   - **Answer:** The softmax function is used to convert the raw output scores into probability distributions, making it suitable for multi-class classification tasks. It ensures that the sum of probabilities for all classes is equal to 1.\n",
    "\n",
    "### Layers and Hidden Layers:\n",
    "\n",
    "5. **Theoretical Question:** Define what a hidden layer is in a neural network.\n",
    "   - **Answer:** A hidden layer is any layer in a neural network that is not an input or output layer. It processes information from the input layer and transforms it before passing it to the next layer.\n",
    "\n",
    "6. **Conceptual Question:** Why are hidden layers necessary in neural networks, and what role do they play?\n",
    "   - **Answer:** Hidden layers enable neural networks to learn complex representations and relationships within the data. They provide the network with the capacity to capture intricate patterns and features.\n",
    "\n",
    "### Batches and Batch Processing:\n",
    "\n",
    "7. **Conceptual Question:** What is the purpose of using batches during the training of a neural network?\n",
    "   - **Answer:** Batches allow the network to process a subset of the training data at a time, making computations more efficient and enabling the use of stochastic gradient descent. It introduces a level of parallelism and can improve convergence.\n",
    "\n",
    "8. **Practical Question:** How does the choice of batch size impact training dynamics, and what factors should be considered when selecting a batch size?\n",
    "   - **Answer:** A larger batch size can result in faster training but requires more memory. Smaller batches may provide more accurate weight updates but can slow down training. The choice depends on factors like available memory, dataset size, and computational resources.\n",
    "\n",
    "### Object Layer:\n",
    "\n",
    "9. **Theoretical Question:** Explain the concept of an object layer in a neural network.\n",
    "   - **Answer:** An object layer refers to an instance or instantiation of a neural network layer within the context of object-oriented programming. It encapsulates the weights, biases, and methods for forward propagation.\n",
    "\n",
    "10. **Practical Question:** What advantages does encapsulating a layer in an object provide compared to a non-object-oriented approach?\n",
    "    - **Answer:** Object-oriented programming allows for modular code, making it easier to manage and reuse layers. It enhances code organization, readability, and maintenance. It also enables the creation of multiple instances with different configurations.\n",
    "\n",
    "These questions cover various aspects of weights, inputs, layers, biases, outputs, batches, hidden layers, and the use of object-oriented programming in the context of neural networks. They provide a mix of theoretical understanding, practical considerations, and conceptual knowledge."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa31d7d85a5ddce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Absolutely! There are numerous aspects and intricacies within neural networks and related topics that can be explored with additional questions. Here are more questions that delve into different aspects of neural networks:\n",
    "\n",
    "### Training and Learning:\n",
    "\n",
    "11. **Conceptual Question:** Explain the terms \"forward pass\" and \"backward pass\" in the context of training a neural network.\n",
    "    - **Answer:** The forward pass involves processing inputs through the network to generate predictions. The backward pass (or backpropagation) is the process of calculating gradients and adjusting weights during training to minimize the loss.\n",
    "\n",
    "12. **Practical Question:** Why is it important to normalize or standardize input data before feeding it into a neural network?\n",
    "    - **Answer:** Normalizing input data helps ensure that features are on a similar scale, preventing some features from dominating others during training. It can lead to more stable and faster convergence.\n",
    "\n",
    "### Activation Functions:\n",
    "\n",
    "13. **Theoretical Question:** Compare the sigmoid and ReLU activation functions. What are their advantages and disadvantages?\n",
    "    - **Answer:** Sigmoid is smooth and suitable for output layers in binary classification, but it can suffer from the vanishing gradient problem. ReLU is computationally efficient and avoids the vanishing gradient problem but can suffer from the dying ReLU problem.\n",
    "\n",
    "14. **Conceptual Question:** What is the purpose of using an activation function in a neural network, and why not use a linear activation for all neurons?\n",
    "    - **Answer:** Activation functions introduce non-linearity, enabling the network to learn complex relationships. Using a linear activation would result in a linear combination of inputs, making it equivalent to a single-layer network, limiting the model's expressiveness.\n",
    "\n",
    "### Overfitting and Regularization:\n",
    "\n",
    "15. **Conceptual Question:** Define overfitting in the context of neural networks. How can regularization techniques like dropout help mitigate overfitting?\n",
    "    - **Answer:** Overfitting occurs when a model learns noise in the training data and performs poorly on new, unseen data. Dropout randomly drops neurons during training, preventing over-reliance on specific neurons and improving generalization.\n",
    "\n",
    "16. **Practical Question:** When should early stopping be applied during training, and how does it help prevent overfitting?\n",
    "    - **Answer:** Early stopping involves halting training when the model's performance on a validation set starts degrading. It prevents the model from becoming overly specialized to the training data, improving generalization to new data.\n",
    "\n",
    "### Optimization Algorithms:\n",
    "\n",
    "17. **Theoretical Question:** Explain the concept of a learning rate in the context of optimization algorithms for training neural networks.\n",
    "    - **Answer:** The learning rate determines the step size during weight updates in gradient descent. A high learning rate can cause oscillations or divergence, while a low learning rate can lead to slow convergence or getting stuck in local minima.\n",
    "\n",
    "18. **Practical Question:** Contrast the advantages and disadvantages of stochastic gradient descent (SGD) and Adam optimization algorithms.\n",
    "    - **Answer:** SGD is computationally efficient but can oscillate. Adam adapts the learning rate for each parameter but may require more memory. The choice depends on factors like dataset size, computational resources, and convergence speed.\n",
    "\n",
    "These additional questions cover topics such as training dynamics, activation functions, regularization techniques, overfitting, and optimization algorithms. The depth and breadth of questions can be tailored based on the specific focus of the interview or the level of expertise expected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9403f15809c57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hidden Layer Activation Functions\n",
    "\n",
    "Activation functions play a crucial role in the hidden layers of neural networks. They introduce non-linearity, enabling the network to learn complex patterns and relationships in the data. Here are some commonly used activation functions for hidden layers:\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit):**\n",
    "   - **Definition:**  ***f(x) = max(0, x)*** \n",
    "   - **Advantages:**\n",
    "     - Simple and computationally efficient.\n",
    "     - Mitigates the vanishing gradient problem.\n",
    "   - **Considerations:**\n",
    "     - May suffer from the \"dying ReLU\" problem, where neurons can become inactive during training.\n",
    "\n",
    "2. **Sigmoid:**\n",
    "   - **Definition:** ***f(x) = {1} / {1 + e^{-x}}***\n",
    "   - **Advantages:**\n",
    "     - Outputs values between 0 and 1, making it suitable for binary classification.\n",
    "   - **Considerations:**\n",
    "     - Prone to vanishing gradient, especially during backpropagation.\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent):**\n",
    "   - **Definition:** ***f(x) = {2} / ( {1 + e^{-2x}} ) - 1***\n",
    "   - **Advantages:**\n",
    "     - Outputs values between -1 and 1, making it zero-centered.\n",
    "     - Mitigates the vanishing gradient problem better than sigmoid.\n",
    "   - **Considerations:**\n",
    "     - Still susceptible to vanishing gradient, but to a lesser extent compared to sigmoid.\n",
    "\n",
    "4. **Leaky ReLU:**\n",
    "   - **Definition:** ***f(x) = max(α * x, x)*** , where ***α***  is a small positive constant.\n",
    "   - **Advantages:**\n",
    "     - Addresses the \"dying ReLU\" problem by allowing a small negative slope for negative inputs.\n",
    "   - **Considerations:**\n",
    "     - The choice of ***α*** is a hyperparameter.\n",
    "\n",
    "5. **Parametric ReLU (PReLU):**\n",
    "   - **Definition:** ***f(x) = max(α * x, x)*** , where ***α*** is a learnable parameter.\n",
    "   - **Advantages:**\n",
    "     - Allows the slope for negative inputs to be learned during training.\n",
    "   - **Considerations:**\n",
    "     - Introduces additional parameters to be learned.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU):**\n",
    "   - **Definition:** ***f(x) = x***  for ***x >= 0 ,  f(x) = α (e^x - 1)*** for ***x < 0*** , where ***α*** is a hyperparameter.\n",
    "   - **Advantages:**\n",
    "     - Smooth for all inputs, preventing dead neurons.\n",
    "     - Can capture negative values without the issues of ReLU.\n",
    "   - **Considerations:**\n",
    "     - Slightly computationally more expensive than ReLU.\n",
    "   \n",
    "7. **Swish:**\n",
    "   - **Definition:** ***f(x) = x ⋅ σ(x)*** , where ***σ*** is the sigmoid function.\n",
    "   - **Advantages:**\n",
    "     - Combines elements of ReLU and sigmoid, often leading to improved performance.\n",
    "   - **Considerations:**\n",
    "     - Introduced as a novel activation function and may require experimentation.\n",
    "\n",
    "8. **Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM):**\n",
    "   - **Advantages:**\n",
    "     - Specifically designed for recurrent neural networks (RNNs).\n",
    "     - Allow networks to capture long-term dependencies in sequences.\n",
    "\n",
    "When choosing an activation function, considerations include the specific characteristics of the task, potential challenges like vanishing gradient problems, and the computational efficiency of the function. The choice might involve experimentation and fine-tuning based on the specific requirements of the neural network and the nature of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84cb4990a1fdf34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e9ba5e62fc441262"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b34041f00482e0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
