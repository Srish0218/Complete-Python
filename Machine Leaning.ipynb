{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "Machine Learning (ML) is a field of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn patterns and make predictions or decisions without being explicitly programmed. The goal of machine learning is to develop systems that can learn from data and improve their performance over time.\n",
    "\n",
    "### Key Concepts in Machine Learning:\n",
    "\n",
    "1. **Types of Machine Learning:**\n",
    "   - **Supervised Learning:** The model is trained on a labeled dataset, where the input data and corresponding output labels are provided. The goal is to learn a mapping from inputs to outputs.\n",
    "   - **Unsupervised Learning:** The model is given unlabeled data, and it tries to find patterns or structures within the data without explicit output labels.\n",
    "   - **Reinforcement Learning:** The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties based on its actions.\n",
    "\n",
    "2. **Types of Models:**\n",
    "   - **Linear Regression:** Used for predicting a continuous outcome based on one or more input features.\n",
    "   - **Logistic Regression:** Used for binary classification problems, where the output is a probability of belonging to a particular class.\n",
    "   - **Decision Trees:** Tree-like models that make decisions based on input features, often used for classification problems.\n",
    "   - **Random Forests:** Ensembles of decision trees, providing higher accuracy and robustness.\n",
    "   - **Support Vector Machines (SVM):** Used for classification and regression tasks by finding a hyperplane that best separates the data points.\n",
    "\n",
    "3. **Neural Networks and Deep Learning:**\n",
    "   - **Artificial Neural Networks (ANN):** Modeled after the human brain, consisting of interconnected nodes (neurons) organized in layers.\n",
    "   - **Deep Learning:** Involves neural networks with multiple hidden layers, enabling the model to learn hierarchical representations of data.\n",
    "\n",
    "4. **Evaluation Metrics:**\n",
    "   - Common metrics for classification problems include accuracy, precision, recall, F1 score, and area under the receiver operating characteristic (ROC) curve.\n",
    "   - For regression problems, metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "5. **Overfitting and Underfitting:**\n",
    "   - **Overfitting:** Occurs when a model performs well on the training data but poorly on new, unseen data.\n",
    "   - **Underfitting:** Occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "### Machine Learning Workflow:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gathering relevant and representative data for training and evaluation.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Cleaning, transforming, and organizing the data to make it suitable for training models.\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - Selecting or creating relevant features (input variables) for the model.\n",
    "\n",
    "4. **Model Selection and Training:**\n",
    "   - Choosing a suitable algorithm and training the model on the training dataset.\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Assessing the model's performance on a separate validation or test dataset.\n",
    "\n",
    "6. **Hyperparameter Tuning:**\n",
    "   - Adjusting model hyperparameters to improve performance.\n",
    "\n",
    "7. **Deployment:**\n",
    "   - Implementing the model in a real-world environment for making predictions on new data.\n",
    "\n",
    "Machine learning is applied in various domains, including image and speech recognition, natural language processing, recommendation systems, autonomous vehicles, and healthcare. Popular machine learning libraries in Python include Scikit-Learn, TensorFlow, and PyTorch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7bd04cd2dbb6da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AI vs ML vs DS\n",
    "\n",
    "\n",
    "AL (Artificial Intelligence), ML (Machine Learning), and DS (Data Science) are related but distinct fields within the broader domain of data and computational intelligence. Here's a brief overview of each:\n",
    "\n",
    "### Artificial Intelligence (AI):\n",
    "\n",
    "1. **Definition:**\n",
    "   - AI refers to the development of computer systems that can perform tasks that typically require human intelligence. It aims to create machines capable of reasoning, problem-solving, perception, understanding natural language, and learning.\n",
    "\n",
    "2. **Scope:**\n",
    "   - AI is a broad field that encompasses various approaches, including rule-based systems, expert systems, symbolic AI, and machine learning.\n",
    "\n",
    "3. **Techniques:**\n",
    "   - In addition to machine learning, AI involves techniques such as natural language processing (NLP), computer vision, robotics, and knowledge representation.\n",
    "\n",
    "4. **Applications:**\n",
    "   - AI applications include virtual assistants, chatbots, image and speech recognition, autonomous vehicles, game playing, and expert systems.\n",
    "\n",
    "### Machine Learning (ML):\n",
    "\n",
    "1. **Definition:**\n",
    "   - ML is a subset of AI that focuses on the development of algorithms and models that allow computers to learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
    "\n",
    "2. **Learning Paradigms:**\n",
    "   - ML includes various learning paradigms such as supervised learning, unsupervised learning, reinforcement learning, and semi-supervised learning.\n",
    "\n",
    "3. **Techniques:**\n",
    "   - ML techniques include linear regression, decision trees, support vector machines, neural networks, clustering, and dimensionality reduction.\n",
    "\n",
    "4. **Applications:**\n",
    "   - ML is applied in diverse domains, including image and speech recognition, recommendation systems, fraud detection, autonomous vehicles, and healthcare.\n",
    "\n",
    "### Data Science (DS):\n",
    "\n",
    "1. **Definition:**\n",
    "   - DS is a multidisciplinary field that involves extracting insights and knowledge from data using a combination of domain knowledge, statistical methods, programming, and machine learning.\n",
    "\n",
    "2. **Components:**\n",
    "   - DS includes data cleaning, exploration, visualization, feature engineering, statistical analysis, and the development of predictive models using machine learning.\n",
    "\n",
    "3. **Skills:**\n",
    "   - Data scientists need a blend of skills in programming (e.g., Python or R), statistics, domain expertise, and machine learning.\n",
    "\n",
    "4. **Applications:**\n",
    "   - DS is applied across industries for tasks such as predictive modeling, business intelligence, fraud detection, optimization, and decision support.\n",
    "\n",
    "### Relationships:\n",
    "\n",
    "- **Overlap:**\n",
    "  - ML is a crucial component of both AI and DS. ML techniques and algorithms are often used in AI systems, and data scientists leverage ML for predictive modeling and analysis.\n",
    "\n",
    "- **Integration:**\n",
    "  - AI and DS can be considered integrated fields, where AI systems often involve the application of DS techniques for data-driven decision-making.\n",
    "\n",
    "In summary, AI is a broader concept that encompasses ML as one of its key components. DS, on the other hand, involves the extraction of insights from data, with ML as a subset of techniques within DS. All three fields are interrelated and contribute to the development of intelligent systems and solutions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22097a5056338322"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of ML\n",
    "\n",
    "Machine learning (ML) is categorized into several types based on the learning approach and the nature of the available data. The three main types of machine learning are:\n",
    "\n",
    "1. **Supervised Learning:**\n",
    "   - **Definition:** In supervised learning, the model is trained on a labeled dataset, where the input data is paired with the corresponding output labels. The goal is to learn a mapping from inputs to outputs.\n",
    "   - **Use Cases:**\n",
    "     - Classification: Predicting the category or class of an input.\n",
    "     - Regression: Predicting a continuous numerical value.\n",
    "   - **Examples:**\n",
    "     - Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees, Neural Networks.\n",
    "\n",
    "2. **Unsupervised Learning:**\n",
    "   - **Definition:** In unsupervised learning, the model is given unlabeled data and is tasked with finding patterns or structures within the data without explicit output labels.\n",
    "   - **Use Cases:**\n",
    "     - Clustering: Grouping similar data points together.\n",
    "     - Dimensionality Reduction: Reducing the number of features while retaining important information.\n",
    "   - **Examples:**\n",
    "     - K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "3. **Reinforcement Learning:**\n",
    "   - **Definition:** Reinforcement learning involves an agent interacting with an environment and learning to make decisions by receiving feedback in the form of rewards or penalties.\n",
    "   - **Components:**\n",
    "     - Agent: The learner or decision-maker.\n",
    "     - Environment: The external system with which the agent interacts.\n",
    "     - Rewards/Penalties: Feedback received by the agent based on its actions.\n",
    "   - **Examples:**\n",
    "     - Q-Learning, Deep Q Networks (DQN), Policy Gradient methods.\n",
    "   - **Use Cases:**\n",
    "     - Game playing, robotics, autonomous systems.\n",
    "\n",
    "Additionally, there are some other specialized types of machine learning:\n",
    "\n",
    "4. **Semi-Supervised Learning:**\n",
    "   - Combines elements of supervised and unsupervised learning. The model is trained on a dataset that contains both labeled and unlabeled examples.\n",
    "\n",
    "5. **Self-Supervised Learning:**\n",
    "   - The model is trained on the data itself to generate labels automatically, without requiring external labeled data.\n",
    "\n",
    "6. **Transfer Learning:**\n",
    "   - Pre-training a model on a large dataset and fine-tuning it for a specific task. This leverages knowledge gained from a source task to improve performance on a target task.\n",
    "\n",
    "7. **Ensemble Learning:**\n",
    "   - Combines the predictions of multiple models to improve overall performance and robustness.\n",
    "\n",
    "8. **Online Learning:**\n",
    "   - The model is continuously updated as new data becomes available, adapting to changing conditions over time.\n",
    "\n",
    "9. **Batch Learning:**\n",
    "   - The model is trained on a fixed dataset, and updates are made periodically after processing batches of data.\n",
    "\n",
    "Understanding the different types of machine learning is crucial for selecting the most appropriate approach for a given task or problem. Each type has its strengths and weaknesses, and the choice depends on the characteristics of the data and the goals of the application."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8bd6264d0de798e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Leaning\n",
    "\n",
    "Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping from inputs to outputs, making it suitable for tasks where the algorithm needs to make predictions or classifications.\n",
    "\n",
    "### Key Components of Supervised Learning:\n",
    "\n",
    "1. **Labeled Dataset:**\n",
    "   - The training dataset consists of examples where each input is associated with a corresponding output label. The labeled data is used to train the model to make predictions on new, unseen data.\n",
    "\n",
    "2. **Training Process:**\n",
    "   - During the training process, the model learns the patterns and relationships between input features and output labels. The goal is to minimize the difference between the predicted outputs and the actual labels.\n",
    "\n",
    "3. **Prediction/Inference:**\n",
    "   - Once trained, the model can make predictions or classifications on new, unseen data. The model generalizes its learned patterns to make accurate predictions on inputs it hasn't seen during training.\n",
    "\n",
    "### Types of Supervised Learning:\n",
    "\n",
    "1. **Classification:**\n",
    "   - **Definition:** In classification, the model predicts the category or class of a given input. The output is a discrete label from a predefined set of classes.\n",
    "   - **Examples:**\n",
    "     - Binary Classification: Predicting whether an email is spam or not.\n",
    "     - Multi-Class Classification: Identifying the type of flower from a set of categories.\n",
    "\n",
    "2. **Regression:**\n",
    "   - **Definition:** In regression, the model predicts a continuous numerical value or quantity. The output is a real number rather than a class label.\n",
    "   - **Examples:**\n",
    "     - Predicting house prices based on features like square footage, number of bedrooms, etc.\n",
    "     - Forecasting stock prices based on historical data.\n",
    "\n",
    "### Steps in Supervised Learning:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gather a labeled dataset that includes input features and corresponding output labels.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Clean and prepare the data, handle missing values, and scale or normalize features if necessary.\n",
    "\n",
    "3. **Feature Selection/Engineering:**\n",
    "   - Select relevant features or create new features to improve the model's performance.\n",
    "\n",
    "4. **Splitting the Data:**\n",
    "   - Split the dataset into training and testing sets to assess the model's performance on unseen data.\n",
    "\n",
    "5. **Choosing a Model:**\n",
    "   - Select an appropriate supervised learning algorithm based on the nature of the task (classification or regression) and the characteristics of the data.\n",
    "\n",
    "6. **Training the Model:**\n",
    "   - Train the model on the training dataset, adjusting the model's parameters to minimize the difference between predicted and actual values.\n",
    "\n",
    "7. **Evaluation:**\n",
    "   - Assess the model's performance on the testing dataset using evaluation metrics appropriate for the task (accuracy, precision, recall, F1 score for classification; mean squared error for regression).\n",
    "\n",
    "8. **Hyperparameter Tuning:**\n",
    "   - Fine-tune the model's hyperparameters to optimize performance.\n",
    "\n",
    "9. **Prediction/Deployment:**\n",
    "   - Deploy the trained model for making predictions on new, unseen data.\n",
    "\n",
    "Popular algorithms for supervised learning include:\n",
    "\n",
    "- **Linear Regression**\n",
    "- **Logistic Regression**\n",
    "- **Support Vector Machines (SVM)**\n",
    "- **Decision Trees**\n",
    "- **Random Forests**\n",
    "- **Gradient Boosting algorithms (e.g., XGBoost, LightGBM)**\n",
    "- **Neural Networks (Deep Learning)**\n",
    "\n",
    "Supervised learning is widely used in various domains, including image and speech recognition, natural language processing, medical diagnosis, finance, and many others.\n",
    "\n",
    "### Features:\n",
    "- **Qualitative** - categorical data (nominal , ordinal)\n",
    "- **Quantitative** - (contunous ,  discrete)\n",
    "\n",
    "### Tasks:\n",
    "1. **Classification** - predict discrete classes\n",
    "   - Binary classification\n",
    "   - Multiclasss classification\n",
    "2. **Regression** - predict continous values\n",
    "\n",
    "\n",
    "Here's a tabular comparison highlighting the key differences between binary classification and multiclass classification:\n",
    "\n",
    "| Feature                             | Binary Classification                       | Multiclass Classification                    |\n",
    "|-------------------------------------|--------------------------------------------|-----------------------------------------------|\n",
    "| **Number of Classes**               | Two (Positive and Negative)               | Three or more (e.g., Class 1, Class 2, Class 3) |\n",
    "| **Output Layer in Neural Networks** | Single neuron with sigmoid activation    | Multiple neurons with softmax activation      |\n",
    "| **Evaluation Metrics**              | Accuracy, Precision, Recall, F1 Score, ROC-AUC | Extended or computed separately for each class |\n",
    "| **Algorithms Handling Multiclass**  | Some algorithms can handle it naturally (e.g., logistic regression, SVM) | May require extensions or specific strategies (one-vs-all, one-vs-one) |\n",
    "| **Examples**                        | Spam detection, Disease diagnosis         | Handwritten digit recognition, Language identification |\n",
    "\n",
    "Understanding these differences is crucial when designing and implementing classification models, as the choice between binary and multiclass classification depends on the specific requirements of the problem at hand.\n",
    "\n",
    "Binary classification and multiclass classification are two types of classification tasks in supervised machine learning. Let's explore each of them:\n",
    "\n",
    "### Binary Classification:\n",
    "\n",
    "**Definition:**\n",
    "Binary classification is a type of supervised learning task where the goal is to categorize input instances into one of two possible classes or categories. The output is a binary decision, often expressed as class labels like \"positive\" and \"negative,\" \"spam\" and \"non-spam,\" or \"1\" and \"0.\"\n",
    "\n",
    "**Examples:**\n",
    "1. Spam Detection: Classifying emails as either spam or not spam.\n",
    "2. Disease Diagnosis: Determining whether a patient has a specific medical condition or not.\n",
    "3. Credit Approval: Approving or rejecting a credit application based on risk assessment.\n",
    "\n",
    "**Algorithms for Binary Classification:**\n",
    "1. Logistic Regression\n",
    "2. Support Vector Machines (SVM)\n",
    "3. Decision Trees and Random Forests\n",
    "4. Naive Bayes\n",
    "5. Neural Networks (with a binary output layer)\n",
    "\n",
    "### Multiclass Classification:\n",
    "\n",
    "**Definition:**\n",
    "Multiclass classification, also known as multinomial classification, is a type of supervised learning task where the goal is to assign input instances to one of three or more classes or categories. Each instance can belong to only one class, and the output is a single class label.\n",
    "\n",
    "**Examples:**\n",
    "1. Handwritten Digit Recognition: Classifying digits (0-9) based on images.\n",
    "2. Species Classification: Identifying the species of a plant or animal from a set of possible categories.\n",
    "3. Language Identification: Determining the language of a given text from multiple language options.\n",
    "\n",
    "**Algorithms for Multiclass Classification:**\n",
    "1. Logistic Regression (Extended for multiclass using one-vs-all or one-vs-one approaches)\n",
    "2. Support Vector Machines (with extensions)\n",
    "3. Decision Trees and Random Forests\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "5. Naive Bayes\n",
    "6. Neural Networks (with a softmax output layer)\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Number of Classes:**\n",
    "   - Binary classification has two classes (positive/negative, spam/non-spam), while multiclass classification has three or more classes.\n",
    "\n",
    "2. **Output Layer in Neural Networks:**\n",
    "   - In neural networks designed for binary classification, the output layer typically has one neuron with a sigmoid activation function. For multiclass classification, the output layer has multiple neurons with a softmax activation function.\n",
    "\n",
    "3. **Evaluation Metrics:**\n",
    "   - For binary classification, metrics include accuracy, precision, recall, F1 score, and ROC-AUC. In multiclass classification, these metrics are often extended or computed separately for each class.\n",
    "\n",
    "4. **Algorithms Handling Multiclass:**\n",
    "   - Some algorithms (like logistic regression and support vector machines) can be extended naturally for multiclass classification, while others may require modifications or the use of specific strategies (one-vs-all, one-vs-one).\n",
    "\n",
    "Both binary and multiclass classification are widely used in various applications, and the choice between them depends on the nature of the problem and the number of distinct classes involved."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6703fd75f964627c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "\n",
    "Regression is a type of supervised machine learning task where the goal is to predict a continuous numerical output or target variable based on one or more input features. The output in regression is a real-valued quantity, making it suitable for tasks such as predicting prices, temperatures, sales, or any other continuous variable.\n",
    "\n",
    "### Key Concepts in Regression:\n",
    "\n",
    "1. **Target Variable:**\n",
    "   - In regression, the variable that we want to predict is called the target variable or dependent variable. It is denoted as \\(y\\).\n",
    "\n",
    "2. **Input Features:**\n",
    "   - The input features, denoted as \\(X\\), are the independent variables used to make predictions about the target variable.\n",
    "\n",
    "3. **Prediction Function:**\n",
    "   - The goal is to learn a mapping or function \\(f\\) that can predict the target variable \\(y\\) based on the input features \\(X\\). The prediction function can be represented as \\(y = f(X)\\).\n",
    "\n",
    "4. **Training Process:**\n",
    "   - During the training process, the model learns the relationships between the input features and the target variable from a labeled dataset.\n",
    "\n",
    "5. **Types of Regression:**\n",
    "   - **Linear Regression:** Assumes a linear relationship between the input features and the target variable. The prediction function is a linear combination of the input features.\n",
    "   - **Polynomial Regression:** Allows for nonlinear relationships by introducing polynomial terms into the regression equation.\n",
    "   - **Ridge Regression and Lasso Regression:** Regularized linear regression methods that prevent overfitting by adding regularization terms.\n",
    "   - **Decision Tree Regression:** Uses decision trees to model complex relationships.\n",
    "   - **Random Forest Regression:** Ensemble of decision trees for improved accuracy and robustness.\n",
    "\n",
    "### Steps in Regression:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gather a dataset with labeled examples, where the input features are associated with corresponding target values.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Clean and preprocess the data, handle missing values, and scale or normalize features if necessary.\n",
    "\n",
    "3. **Feature Selection/Engineering:**\n",
    "   - Select relevant features or create new features to improve the model's performance.\n",
    "\n",
    "4. **Splitting the Data:**\n",
    "   - Divide the dataset into training and testing sets to assess the model's performance on unseen data.\n",
    "\n",
    "5. **Choosing a Model:**\n",
    "   - Select an appropriate regression algorithm based on the nature of the task and the characteristics of the data.\n",
    "\n",
    "6. **Training the Model:**\n",
    "   - Train the model on the training dataset, adjusting the model's parameters to minimize the difference between predicted and actual values.\n",
    "\n",
    "7. **Evaluation:**\n",
    "   - Assess the model's performance on the testing dataset using evaluation metrics like mean squared error (MSE), mean absolute error (MAE), or \\(R^2\\) score.\n",
    "\n",
    "8. **Hyperparameter Tuning:**\n",
    "   - Fine-tune the model's hyperparameters to optimize performance.\n",
    "\n",
    "9. **Prediction/Deployment:**\n",
    "   - Deploy the trained model for making predictions on new, unseen data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a simple linear regression task where we want to predict house prices based on the square footage of the house. The dataset includes pairs of (square footage, house price), and the goal is to learn a model that can predict house prices for new houses.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "square_footage = np.array([1400, 1600, 1700, 1875, 1100, 1550, 2350, 2450, 1425, 1700])\n",
    "house_prices = np.array([245000, 312000, 279000, 308000, 199000, 219000, 405000, 324000, 319000, 255000])\n",
    "\n",
    "# Reshape the data\n",
    "square_footage = square_footage.reshape((-1, 1))\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(square_footage, house_prices)\n",
    "\n",
    "# Make predictions for new houses\n",
    "new_square_footage = np.array([1600, 1800, 2000]).reshape((-1, 1))\n",
    "predicted_prices = model.predict(new_square_footage)\n",
    "\n",
    "print(\"Predicted Prices:\", predicted_prices)\n",
    "```\n",
    "\n",
    "This is a simplified example, but it illustrates the basic steps involved in a regression task. The model learns a linear relationship between square footage and house prices, allowing it to make predictions for new houses.\n",
    "\n",
    "### Loss Function \n",
    "loss = sum( | Y(real) - Y(predicted) |)\n",
    "\n",
    "A loss function, also known as a cost function or objective function, is a key component in the training of machine learning models. It quantifies the difference between the predicted values generated by the model and the actual values (ground truth) present in the training dataset. The goal during training is to minimize this loss function, making the model's predictions as close as possible to the actual values.\n",
    "\n",
    "The choice of a loss function depends on the type of task the machine learning model is designed to perform (e.g., classification, regression). Here are some commonly used loss functions for different tasks:\n",
    "\n",
    "### Regression Loss Functions:\n",
    "\n",
    "1. **Mean Squared Error (MSE):**\n",
    "   - **Definition:** MSE is the average of the squared differences between predicted and actual values.\n",
    "   - **Formula:** \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "   - **Use Case:** Commonly used in linear regression and other regression tasks.\n",
    "\n",
    "2. **Mean Absolute Error (MAE):**\n",
    "   - **Definition:** MAE is the average of the absolute differences between predicted and actual values.\n",
    "   - **Formula:** \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n",
    "   - **Use Case:** Provides a more interpretable measure of error compared to MSE.\n",
    "\n",
    "3. **Huber Loss:**\n",
    "   - **Definition:** Combines the characteristics of MSE and MAE, providing a balance between robustness and sensitivity to outliers.\n",
    "\n",
    "### Classification Loss Functions:\n",
    "\n",
    "1. **Binary Cross-Entropy Loss (Log Loss):**\n",
    "   - **Definition:** Used in binary classification problems. Measures the average negative log-likelihood of the true labels given the predicted probabilities.\n",
    "   - **Formula:** \\[ \\text{Binary Cross-Entropy} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) \\]\n",
    "\n",
    "2. **Categorical Cross-Entropy Loss (Softmax Loss):**\n",
    "   - **Definition:** Used in multiclass classification problems. Extends binary cross-entropy to more than two classes.\n",
    "   - **Formula:** \\[ \\text{Categorical Cross-Entropy} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij}) \\]\n",
    "     - \\(C\\) is the number of classes, and \\(y_{ij}\\) is 1 if the true class of example \\(i\\) is \\(j\\) and 0 otherwise.\n",
    "\n",
    "3. **Hinge Loss (SVM Loss):**\n",
    "   - **Definition:** Used in support vector machines (SVM) for binary classification. Encourages correct classification with a margin.\n",
    "   - **Formula:** \\[ \\text{Hinge Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\]\n",
    "\n",
    "### Other Loss Functions:\n",
    "\n",
    "1. **Kullback-Leibler Divergence (KL Divergence):**\n",
    "   - **Definition:** Measures how one probability distribution diverges from a second, expected probability distribution. Used in tasks such as variational autoencoders (VAEs).\n",
    "\n",
    "2. **Triplet Loss:**\n",
    "   - **Definition:** Used in triplet networks for tasks like face recognition. Encourages the model to reduce the distance between similar instances and increase the distance between dissimilar ones.\n",
    "\n",
    "Choosing an appropriate loss function is essential for the success of a machine learning model, as it guides the training process towards learning meaningful patterns in the data. The specific characteristics of the task and the model architecture influence the choice of the loss function.\n",
    "### Mean Squared Error (MSE)\n",
    "L2 loss, also known as Mean Squared Error (MSE) or Euclidean loss, is a common loss function used in regression tasks. It quantifies the average squared difference between predicted values and actual values. The formula for L2 loss or MSE is as follows:\n",
    "\n",
    "\\[ \\text{L2 Loss (MSE)} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "Here:\n",
    "- \\( n \\) is the number of examples in the dataset.\n",
    "- \\( y_i \\) is the actual or true value for the \\( i \\)-th example.\n",
    "- \\( \\hat{y}_i \\) is the predicted value for the \\( i \\)-th example.\n",
    "\n",
    "The L2 loss penalizes larger errors more heavily than smaller errors because it squares the differences. Minimizing the L2 loss during training corresponds to minimizing the average squared difference between predictions and actual values.\n",
    "\n",
    "### Characteristics of L2 Loss:\n",
    "\n",
    "1. **Squared Differences:**\n",
    "   - L2 loss computes the squared differences between predicted and actual values.\n",
    "\n",
    "2. **Differentiability:**\n",
    "   - The L2 loss function is differentiable, allowing the use of gradient-based optimization algorithms like gradient descent for model training.\n",
    "\n",
    "3. **Sensitivity to Outliers:**\n",
    "   - L2 loss is sensitive to outliers. Large errors have a substantial impact on the loss, and the model may be overly influenced by outliers.\n",
    "\n",
    "### Usage in Regression:\n",
    "\n",
    "L2 loss is commonly used in regression problems, including linear regression and neural network regression tasks. In linear regression, the goal is to find the line that minimizes the sum of squared differences between the predicted and actual values.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "actual_values = np.array([2, 4, 6, 8, 10])\n",
    "predicted_values = np.array([1.8, 3.5, 6.3, 8.2, 9.7])\n",
    "\n",
    "# Calculate L2 loss (MSE) using NumPy\n",
    "mse = np.mean((actual_values - predicted_values)**2)\n",
    "\n",
    "# Alternatively, calculate L2 loss using scikit-learn\n",
    "mse_sklearn = mean_squared_error(actual_values, predicted_values)\n",
    "\n",
    "print(\"L2 Loss (MSE) using NumPy:\", mse)\n",
    "print(\"L2 Loss (MSE) using scikit-learn:\", mse_sklearn)\n",
    "```\n",
    "\n",
    "In the above example, lower L2 loss values indicate a better fit of the model to the data.\n",
    "\n",
    "### Note:\n",
    "While L2 loss is widely used, it's essential to be aware of its sensitivity to outliers. In the presence of outliers, other loss functions like Huber loss or robust regression may be considered to provide a more balanced approach to handling extreme values.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is a commonly used loss function in regression tasks. It measures the average absolute difference between predicted values and actual values. The formula for MAE is as follows:\n",
    "\n",
    "\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n",
    "\n",
    "Here:\n",
    "- \\( n \\) is the number of examples in the dataset.\n",
    "- \\( y_i \\) is the actual or true value for the \\( i \\)-th example.\n",
    "- \\( \\hat{y}_i \\) is the predicted value for the \\( i \\)-th example.\n",
    "\n",
    "### Characteristics of MAE:\n",
    "\n",
    "1. **Absolute Differences:**\n",
    "   - MAE computes the absolute differences between predicted and actual values.\n",
    "\n",
    "2. **Robustness to Outliers:**\n",
    "   - Unlike Mean Squared Error (MSE), MAE is less sensitive to outliers. It provides a more balanced measure of error in the presence of extreme values.\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - MAE is more interpretable than MSE because it represents the average absolute error, which is in the same units as the target variable.\n",
    "\n",
    "### Usage in Regression:\n",
    "\n",
    "MAE is particularly useful in situations where outliers may have a significant impact on the model's performance, and a more robust loss function is desired. It is suitable for tasks where the emphasis is on the magnitude of errors rather than their squared values.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Example data\n",
    "actual_values = np.array([2, 4, 6, 8, 10])\n",
    "predicted_values = np.array([1.8, 3.5, 6.3, 8.2, 9.7])\n",
    "\n",
    "# Calculate MAE using NumPy\n",
    "mae = np.mean(np.abs(actual_values - predicted_values))\n",
    "\n",
    "# Alternatively, calculate MAE using scikit-learn\n",
    "mae_sklearn = mean_absolute_error(actual_values, predicted_values)\n",
    "\n",
    "print(\"MAE using NumPy:\", mae)\n",
    "print(\"MAE using scikit-learn:\", mae_sklearn)\n",
    "```\n",
    "\n",
    "In the above example, lower MAE values indicate a better fit of the model to the data. The interpretation of MAE is straightforward, as it represents the average absolute error across all examples.\n",
    "\n",
    "### Note:\n",
    "While MAE is robust to outliers, it might give equal weight to all errors, including small ones. In scenarios where certain errors are more critical than others, other loss functions like Huber loss or quantile loss may be considered.\n",
    "\n",
    "\n",
    "### Binary Cross-Entropy Loss (Log Loss)\n",
    "\n",
    "Binary Cross-Entropy Loss, commonly known as Log Loss, is a loss function used in binary classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1. The formula for Binary Cross-Entropy Loss is as follows:\n",
    "\n",
    "\\[ \\text{Binary Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) \\]\n",
    "\n",
    "Here:\n",
    "- \\( n \\) is the number of examples in the dataset.\n",
    "- \\( y_i \\) is the actual label for the \\( i \\)-th example (0 or 1).\n",
    "- \\( \\hat{y}_i \\) is the predicted probability that the example belongs to class 1.\n",
    "\n",
    "### Characteristics of Binary Cross-Entropy Loss:\n",
    "\n",
    "1. **Probability Interpretation:**\n",
    "   - The loss is based on the logarithm of predicted probabilities, penalizing models more when they confidently predict the wrong class.\n",
    "\n",
    "2. **Differentiability:**\n",
    "   - Binary Cross-Entropy Loss is differentiable, making it suitable for optimization using gradient-based methods.\n",
    "\n",
    "### Usage in Binary Classification:\n",
    "\n",
    "Binary Cross-Entropy Loss is commonly used as the loss function for binary classification problems, where the goal is to predict whether an instance belongs to one of two classes (0 or 1). The loss function is particularly well-suited for models that output probabilities, such as logistic regression or neural networks with a sigmoid activation function in the output layer.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Example data\n",
    "actual_labels = np.array([1, 0, 1, 1, 0])\n",
    "predicted_probabilities = np.array([0.8, 0.3, 0.9, 0.65, 0.2])\n",
    "\n",
    "# Calculate Binary Cross-Entropy Loss using NumPy\n",
    "log_loss_value = -np.mean(actual_labels * np.log(predicted_probabilities) + (1 - actual_labels) * np.log(1 - predicted_probabilities))\n",
    "\n",
    "# Alternatively, calculate Binary Cross-Entropy Loss using scikit-learn\n",
    "log_loss_sklearn = log_loss(actual_labels, predicted_probabilities)\n",
    "\n",
    "print(\"Binary Cross-Entropy Loss using NumPy:\", log_loss_value)\n",
    "print(\"Binary Cross-Entropy Loss using scikit-learn:\", log_loss_sklearn)\n",
    "```\n",
    "\n",
    "In the above example, lower Binary Cross-Entropy Loss values indicate a better fit of the model to the data. The loss is minimized when the predicted probabilities align well with the actual labels.\n",
    "\n",
    "### Note:\n",
    "Binary Cross-Entropy Loss is an important loss function for binary classification, and it is commonly used as an optimization objective during the training of classification models.\n",
    "\n",
    "## Metrics of Performance\n",
    "\n",
    "Performance metrics are used to evaluate the effectiveness of machine learning models. The choice of metrics depends on the type of task (classification, regression, clustering, etc.) and the specific goals of the model. Here are some commonly used performance metrics for different tasks:\n",
    "\n",
    "### Classification Metrics:\n",
    "\n",
    "1. **Accuracy:**\n",
    "   - **Definition:** Proportion of correctly classified instances out of the total number of instances.\n",
    "   - **Formula:** \\[ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\]\n",
    "\n",
    "2. **Precision:**\n",
    "   - **Definition:** Proportion of true positive predictions out of the total positive predictions.\n",
    "   - **Formula:** \\[ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}} \\]\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):**\n",
    "   - **Definition:** Proportion of true positive predictions out of the total actual positives.\n",
    "   - **Formula:** \\[ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} \\]\n",
    "\n",
    "4. **F1 Score:**\n",
    "   - **Definition:** Harmonic mean of precision and recall.\n",
    "   - **Formula:** \\[ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}} \\]\n",
    "\n",
    "5. **Area Under the Receiver Operating Characteristic (ROC-AUC):**\n",
    "   - **Definition:** Area under the ROC curve, which represents the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "6. **Area Under the Precision-Recall Curve (PR AUC):**\n",
    "   - **Definition:** Area under the precision-recall curve, useful for imbalanced datasets.\n",
    "\n",
    "### Regression Metrics:\n",
    "\n",
    "1. **Mean Absolute Error (MAE):**\n",
    "   - **Definition:** Average absolute difference between predicted and actual values.\n",
    "   - **Formula:** \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n",
    "\n",
    "2. **Mean Squared Error (MSE):**\n",
    "   - **Definition:** Average squared difference between predicted and actual values.\n",
    "   - **Formula:** \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):**\n",
    "   - **Definition:** Square root of the MSE.\n",
    "   - **Formula:** \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} \\]\n",
    "\n",
    "4. **R-squared (Coefficient of Determination):**\n",
    "   - **Definition:** Proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
    "   - **Formula:** \\[ R^2 = 1 - \\frac{\\text{Sum of Squared Residuals}}{\\text{Total Sum of Squares}} \\]\n",
    "\n",
    "### Clustering Metrics:\n",
    "\n",
    "1. **Silhouette Score:**\n",
    "   - **Definition:** Measures how similar an object is to its own cluster compared to other clusters.\n",
    "   - **Range:** [-1, 1], where a higher score indicates better-defined clusters.\n",
    "\n",
    "2. **Calinski-Harabasz Index:**\n",
    "   - **Definition:** Ratio of the between-cluster variance to the within-cluster variance.\n",
    "   - **Higher Values:** Indicate better-defined clusters.\n",
    "\n",
    "### Other Metrics:\n",
    "\n",
    "1. **Mean Squared Logarithmic Error (MSLE):**\n",
    "   - **Definition:** Measures the mean squared logarithmic difference between the predicted and actual values.\n",
    "\n",
    "2. **Cohen's Kappa:**\n",
    "   - **Definition:** Measures the agreement between predicted and actual classifications, adjusted for chance.\n",
    "\n",
    "3. **Jaccard Index (Intersection over Union):**\n",
    "   - **Definition:** Measures the similarity between two sets, often used in image segmentation tasks.\n",
    "\n",
    "4. **Matthews Correlation Coefficient (MCC):**\n",
    "   - **Definition:** Measures the correlation between predicted and actual binary classifications.\n",
    "\n",
    "The choice of metrics depends on the specific goals of the machine learning task, and it is common to use a combination of metrics to provide a comprehensive evaluation of model performance.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdf510e76d73878b"
  },
  {
   "cell_type": "markdown",
   "source": [
    " Machine Learning Introduction ðŸ“š\n",
    "\n",
    "What is Machine Learning? ðŸ“š\n",
    "Author:- Ayush Singh [ Newera ( https://youtube.com/c/neweraa ] ðŸ“š\n",
    "Computer programs that uses algorithms to analyze data and make intelligent predictions based on the data without being explicitly programmed.\n",
    "\n",
    "Slightly more formal definition:-\n",
    "\n",
    "Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "Arthur Samuel, 1959\n",
    "\n",
    "A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "\n",
    "Tom Mitchell, 1997\n",
    "\n",
    "Applications of Machine Learning:-\n",
    "\n",
    "Self Driving Cars\n",
    "\n",
    "In Real Estate\n",
    "\n",
    "Stock Price Prediction\n",
    "\n",
    "Much More...\n",
    "\n",
    ".\n",
    "\n",
    "How it works?ðŸ“š\n",
    "Study in the Problem\n",
    "\n",
    "Train the Algorithm\n",
    "\n",
    "Evaluate it\n",
    "\n",
    "If it is Good\n",
    "\n",
    "Launch it\n",
    "\n",
    "Else, Analyze the errors\n",
    "\n",
    "\n",
    "Types of Machine Learning Systems ðŸ“š\n",
    "\n",
    "\n",
    "Supervised Learning:- In this, we feed data to the algorithms, In which the data are labeled and we know what our output should like having the relationship between input values \"X\" and Output values \"Y\".\n",
    "\n",
    "Un Supervised Learning:- In this, we feed data to the algorithms which are not labeled or we can say that we don't know what our output should look like and there is not any kind of relationship between input var and output var. We have to recognize patterns based on the data, for doing so, we have different algorithms, we will study them later.\n",
    "\n",
    "\n",
    "Supervised Learning Problems ðŸ“š\n",
    "Regression:- for continuous data\n",
    "\n",
    "A person's height: could be any value (within the range of human heights), not just certain fixed heights.\n",
    "\n",
    "Classification:- for categorical data\n",
    "\n",
    "Discrete data is counted.\n",
    "\n",
    "\n",
    "Dividing Our Data ðŸ“š\n",
    "We divide our data into two sets, Training and Testing sets the reason is very simple, we want to test the model after we are done, so we make a test set for evaluating our model.\n",
    "\n",
    "\n",
    "Overfitting the training data ðŸ“š\n",
    "It means that the model performs well on the training data, but it does not generalize well.\n",
    "\n",
    "Solution to this problem is To gather more data, To reduce the noise in the training data, To use regularization ( we will study later on )\n",
    "\n",
    "\n",
    "Underfitting the Training Data ðŸ“š\n",
    "It means that the model performs bad on the trainig data,so it's obvious that it will also perform badly on testing data.\n",
    "\n",
    "\n",
    "\n",
    "Notations:- ðŸ“š\n",
    "X means input features.\n",
    "\n",
    "y means Output features.\n",
    "\n",
    "m means no. of training examples."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805dc05c60c550b8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b586bfb20eda64b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5e57d6b37656c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
