{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "A Neural Network is a machine learning model inspired by the structure and functioning of the human brain. It is composed of interconnected nodes, known as neurons, organized into layers. Neural networks have the ability to learn complex relationships and patterns from data, making them versatile for various tasks, including image and speech recognition, natural language processing, and regression.\n",
    "\n",
    "### Key Concepts of Neural Networks:\n",
    "\n",
    "1. **Neurons:**\n",
    "   - Neurons are the basic units of a neural network. Each neuron receives input, applies an activation function, and produces an output.\n",
    "\n",
    "2. **Layers:**\n",
    "   - Neural networks are organized into layers:\n",
    "     - **Input Layer:** Receives the initial input features.\n",
    "     - **Hidden Layers:** Layers between the input and output layers. Deep neural networks have multiple hidden layers.\n",
    "     - **Output Layer:** Produces the final output.\n",
    "\n",
    "3. **Weights and Biases:**\n",
    "   - Each connection between neurons has an associated weight, which is adjusted during training. Biases are added to the inputs before the activation function.\n",
    "\n",
    "4. **Activation Function:**\n",
    "   - The activation function introduces non-linearity into the network, allowing it to learn complex relationships. Common activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).\n",
    "\n",
    "5. **Forward Propagation:**\n",
    "   - During forward propagation, input features are passed through the network layer by layer, producing the final output.\n",
    "\n",
    "6. **Backpropagation:**\n",
    "   - Backpropagation is an optimization algorithm used to update the weights and biases based on the error between predicted and actual outputs. It involves calculating gradients and applying gradient descent.\n",
    "\n",
    "### Types of Neural Networks:\n",
    "\n",
    "1. **Feedforward Neural Network (FNN):**\n",
    "   - The simplest type, where information flows in one direction, from input to output.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN):**\n",
    "   - Specialized for image processing, with convolutional layers that capture spatial hierarchies.\n",
    "\n",
    "3. **Recurrent Neural Network (RNN):**\n",
    "   - Designed for sequence data, capable of handling input sequences of varying lengths.\n",
    "\n",
    "4. **Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU):**\n",
    "   - Variants of RNNs designed to address the vanishing gradient problem in longer sequences.\n",
    "\n",
    "5. **Autoencoders:**\n",
    "   - Neural networks used for unsupervised learning and dimensionality reduction.\n",
    "\n",
    "### Training a Neural Network:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize weights and biases randomly.\n",
    "\n",
    "2. **Forward Propagation:**\n",
    "   - Pass input through the network to generate predictions.\n",
    "\n",
    "3. **Calculate Loss:**\n",
    "   - Measure the difference between predicted and actual outputs using a loss function.\n",
    "\n",
    "4. **Backpropagation:**\n",
    "   - Calculate gradients of the loss with respect to the weights and biases.\n",
    "\n",
    "5. **Update Weights and Biases:**\n",
    "   - Adjust weights and biases in the direction that reduces the loss using optimization algorithms like gradient descent.\n",
    "\n",
    "6. **Repeat:**\n",
    "   - Iteratively perform forward and backward passes until the model converges to a satisfactory solution.\n",
    "\n",
    "### Example Using Keras (a high-level neural networks API in Python):\n",
    "\n",
    "Here's a simple example of a feedforward neural network using Keras for binary classification:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple feedforward neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=20))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "This example demonstrates how to create a simple feedforward neural network using Keras, compile it with an optimizer and loss function, train it on synthetic data, and evaluate its accuracy on a test set. The model consists of one hidden layer with a ReLU activation function and an output layer with a sigmoid activation function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80018280ca3f11f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "An activation function is a crucial component in a neural network, as it introduces non-linearity into the model. Non-linearity allows the neural network to learn complex patterns and relationships in data. The activation function is applied to the output of each neuron (or node) in the network.\n",
    "\n",
    "Here are some commonly used activation functions in neural networks:\n",
    "\n",
    "1. **Sigmoid Activation Function:**\n",
    "   - **Formula:** \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - **Range:** (0, 1)\n",
    "   - **Use Case:** Often used in the output layer for binary classification problems, where the goal is to predict probabilities.\n",
    "\n",
    "2. **Hyperbolic Tangent (tanh) Activation Function:**\n",
    "   - **Formula:** \\( \\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1} \\)\n",
    "   - **Range:** (-1, 1)\n",
    "   - **Use Case:** Similar to the sigmoid, but with a range from -1 to 1. Commonly used in hidden layers.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU) Activation Function:**\n",
    "   - **Formula:** \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "   - **Range:** [0, \\(\\infty\\))\n",
    "   - **Use Case:** Very popular in hidden layers due to simplicity and effectiveness. Can help with mitigating the vanishing gradient problem.\n",
    "\n",
    "4. **Leaky ReLU Activation Function:**\n",
    "   - **Formula:** \\( \\text{Leaky ReLU}(x) = \\max(\\alpha x, x) \\) where \\(\\alpha\\) is a small positive constant (e.g., 0.01).\n",
    "   - **Range:** \\((-\\infty, \\infty)\\)\n",
    "   - **Use Case:** A variant of ReLU that addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient for negative inputs.\n",
    "\n",
    "5. **Softmax Activation Function:**\n",
    "   - **Formula:** \\( \\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}} \\) for \\(i = 1, 2, ..., K\\), where \\(K\\) is the number of classes.\n",
    "   - **Range:** (0, 1) for each element, and the sum of all elements is 1.\n",
    "   - **Use Case:** Commonly used in the output layer for multi-class classification problems, as it converts raw scores into probabilities.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU) Activation Function:**\n",
    "   - **Formula:** \\( \\text{ELU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha (e^x - 1), & \\text{if } x \\leq 0 \\end{cases} \\) where \\(\\alpha\\) is a small positive constant (e.g., 1.0).\n",
    "   - **Range:** \\((-\\infty, \\infty)\\)\n",
    "   - **Use Case:** Similar to ReLU, but with a smooth transition for negative values, helping with vanishing gradient.\n",
    "\n",
    "Choosing the right activation function depends on the specific characteristics of the problem at hand. Experimentation and testing different activation functions can help determine which one works best for a particular neural network architecture and task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73f5998bd983d9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Backpropagation (short for \"backward propagation of errors\") is an optimization algorithm used to train artificial neural networks. It is a supervised learning technique that aims to minimize the error between the predicted output and the actual output for a given set of input data. Backpropagation is a key component of training neural networks, and it involves updating the weights and biases of the network based on the calculated gradients.\n",
    "\n",
    "Here are the main steps involved in the backpropagation algorithm:\n",
    "\n",
    "1. **Forward Propagation:**\n",
    "   - The input data is fed forward through the neural network to produce a predicted output. During this process, the network's weights and biases are used to calculate the weighted sum of inputs and apply an activation function for each neuron in each layer.\n",
    "\n",
    "2. **Calculate Loss:**\n",
    "   - The difference between the predicted output and the actual output is calculated using a loss function. The loss function measures how far off the predictions are from the true values.\n",
    "\n",
    "3. **Backward Propagation:**\n",
    "   - The gradient of the loss with respect to the weights and biases is calculated for each layer using the chain rule of calculus. This involves computing the partial derivatives of the loss with respect to the outputs of each neuron, and then applying the chain rule to find the derivatives with respect to the weights and biases.\n",
    "\n",
    "4. **Gradient Descent (or another optimization algorithm):**\n",
    "   - The weights and biases of the network are updated to minimize the loss. This is typically done using an optimization algorithm such as gradient descent. The update rule involves subtracting a fraction of the gradient from the current weights and biases.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Steps 1-4 are repeated for multiple iterations (epochs) until the model's performance converges to a satisfactory level.\n",
    "\n",
    "6. **Learning Rate:**\n",
    "   - The learning rate is a hyperparameter that determines the size of the steps taken during the weight and bias updates. A too large or too small learning rate can affect the convergence of the training process.\n",
    "\n",
    "The backpropagation algorithm essentially adjusts the parameters of the neural network in the opposite direction of the gradient of the loss function with respect to those parameters. This process continues iteratively, updating the weights and biases until the model achieves a satisfactory level of accuracy on the training data.\n",
    "\n",
    "Here's a simplified example of backpropagation using gradient descent in Python:\n",
    "\n",
    "```python\n",
    "# Assuming a simple feedforward neural network with one hidden layer\n",
    "# and a mean squared error loss function\n",
    "\n",
    "# Forward propagation\n",
    "# ...\n",
    "\n",
    "# Calculate loss\n",
    "loss = mean_squared_error(predicted_output, actual_output)\n",
    "\n",
    "# Backward propagation\n",
    "# Compute gradients of loss with respect to weights and biases using the chain rule\n",
    "\n",
    "# Update weights and biases using gradient descent\n",
    "learning_rate = 0.01\n",
    "weights -= learning_rate * gradient_weights\n",
    "biases -= learning_rate * gradient_biases\n",
    "```\n",
    "\n",
    "In practice, deep learning frameworks like TensorFlow or PyTorch automate the process of backpropagation, making it easier for practitioners to build and train neural networks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f686eb36f8931fa"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T14:20:13.387746Z",
     "start_time": "2024-01-13T14:20:13.377071300Z"
    }
   },
   "id": "cdaea6d5fa4826c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73f1ee5b1f6e80be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
