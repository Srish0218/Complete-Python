{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and widely used algorithm for both classification and regression tasks in machine learning. It is a type of instance-based learning where the model memorizes the entire training dataset and makes predictions based on the similarity between new instances and existing instances in the training set. The key idea is that instances with similar features tend to belong to the same class or have similar output values.\n",
    "\n",
    "### Key Concepts of KNN:\n",
    "\n",
    "1. **K-Nearest Neighbors:**\n",
    "   - The \"K\" in KNN represents the number of nearest neighbors to consider when making predictions. The value of K is a hyperparameter that needs to be specified before training the model.\n",
    "\n",
    "2. **Distance Metric:**\n",
    "   - The choice of distance metric (e.g., Euclidean distance, Manhattan distance, Minkowski distance) determines how the algorithm measures the similarity between instances.\n",
    "\n",
    "### KNN for Classification:\n",
    "\n",
    "1. **Training:**\n",
    "   - KNN stores the entire training dataset.\n",
    "\n",
    "2. **Prediction:**\n",
    "   - To make a prediction for a new instance, KNN identifies the K nearest neighbors in the training set based on the chosen distance metric.\n",
    "\n",
    "3. **Voting (Classification):**\n",
    "   - For classification, the algorithm counts the number of neighbors in each class and assigns the class with the majority of votes to the new instance.\n",
    "\n",
    "### KNN for Regression:\n",
    "\n",
    "1. **Training:**\n",
    "   - KNN stores the entire training dataset along with corresponding output values.\n",
    "\n",
    "2. **Prediction:**\n",
    "   - To make a prediction for a new instance, KNN identifies the K nearest neighbors in the training set based on the chosen distance metric.\n",
    "\n",
    "3. **Averaging (Regression):**\n",
    "   - For regression, the algorithm takes the average of the output values of the K nearest neighbors and assigns it as the predicted output for the new instance.\n",
    "\n",
    "### Hyperparameter: K\n",
    "\n",
    "The choice of the hyperparameter K is crucial in KNN. A smaller K may lead to a more sensitive model, which might be influenced by noise, while a larger K may lead to a smoother decision boundary. The optimal value of K depends on the specific characteristics of the dataset.\n",
    "\n",
    "### Pros and Cons of KNN:\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- No training phase; the model is simply a memorization of the training data.\n",
    "- Effective for small to medium-sized datasets.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive during prediction, especially for large datasets.\n",
    "- Sensitive to irrelevant or redundant features.\n",
    "- The choice of distance metric can impact performance.\n",
    "- Not suitable for high-dimensional data.\n",
    "\n",
    "### Example Using Scikit-Learn:\n",
    "\n",
    "Here's a simple example using the famous Iris dataset for classification:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with K=3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "This example demonstrates how to create a KNN classifier using scikit-learn, train it on the Iris dataset, and evaluate its accuracy on a test set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33bfc4b5babc8db1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Euclidean Distance\n",
    "The Euclidean distance between two points in a Euclidean space is a measure of the straight-line distance between them. For two points \\( (x_1, y_1) \\) and \\( (x_2, y_2) \\) in a two-dimensional space, the Euclidean distance \\( d \\) is given by the formula:\n",
    "\n",
    "\\[ d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\]\n",
    "\n",
    "In general, for points in an \\( n \\)-dimensional space \\( (x_1, x_2, ..., x_n) \\) and \\( (y_1, y_2, ..., y_n) \\), the Euclidean distance \\( d \\) is given by:\n",
    "\n",
    "\\[ d = \\sqrt{\\sum_{i=1}^{n} (y_i - x_i)^2} \\]\n",
    "\n",
    "In machine learning, the Euclidean distance is commonly used as a distance metric for measuring the similarity between instances. In the context of K-Nearest Neighbors (KNN), for example, it is used to identify the K nearest neighbors to a given data point based on their feature values.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee2faceeaa26635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "df7ec311c0a60280"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
